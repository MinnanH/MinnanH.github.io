---
layout:     post
title:     强化学习入门
subtitle:   扫盲
date:       2024-12-04
author:     MinnanH
header-img: img/home-bg-o.jpg
catalog: 	  true
tags:
    - 数据分析
---
自学了一套b站上的强化学习入门课，用来了解强化学习基本思想足够了。
<a href="https://www.bilibili.com/video/BV13a4y1J7bw">课程链接</a>

## 要点总结
#### 1. 强化学习的定义
- 强化学习是 agent（主体/玩家）与环境（environment）互动的过程中，为了达成某一目标（goal）而进行的学习过程。
- 关键特性：goal-directed learning from interaction with the environment。

#### 2. 强化学习的三层结构

(1) 基本元素
- Agent（主体/玩家）：在环境中采取行动的主体。
- Environment（环境）：与 agent 交互的场景。
- Goal（目标）：agent 努力达成的目的。

(2) 主要元素
- State（状态）：当前环境的描述信息，反映游戏的所有相关内容。
- Action（行动）：agent 在某状态下采取的行为。
- Reward（奖励）：agent 采取行动后得到的反馈，通常为数值，用来衡量行动的好坏。

(3) 核心元素
- Policy（策略）：决定 agent 在某状态下应采取的行动，是一个从状态到行动的函数。
- Value（价值）：
  - State Value（状态价值函数）：表示从某状态出发，未来能获得的预期总奖励。
  - State-Action Value（状态-行动价值函数）：表示从某状态采取某行动后，未来能获得的预期总奖励。

#### 3. 强化学习的目标
- 最大化奖励的总和（长远的目标）。
- 通过设计合理的奖励机制，优化 agent 的策略，使其能够完成目标。

#### 4. 关系与关键点
- 主要元素（State、Action、Reward）构成了强化学习的基础交互。
- 核心元素（Policy、Value）是强化学习的优化方向：
  - 好的 Value 函数决定好的 Policy。
  - 一些算法可以直接学习 Policy，但大部分强化学习算法是基于 Value 的。

#### 5. 强化学习的特点
- 强调通过交互和反馈学习。
- 与其他机器学习方法相比，目标导向明确，强调长远利益和短期反馈的平衡。

#### 6. 实际应用
- 强化学习广泛应用于游戏领域（如围棋、DOTA、星际争霸等）。
- 其核心思想也适用于其他需要长期决策优化的场景。

#### 结论：强化学习的本质是通过不断优化策略和价值函数，使 agent 在复杂环境中能够自主决策并实现目标。

## 课程逐字稿
首先我们应该对强化学习有一个基本的了解。那么我认为最好的办法就是去分析一下强化学习的结构或者说组成元素。

首先，什么是强化学习？强化学习是在与环境的互动当中，为了达成一个目标而进行的学习过程，因此称之为goal-directed learning from interaction with the environment。这其实就是强化学习的第一层结构，我称之为基本元素，包括：第一，agent；第二，environment；第三，goal。environment是环境，goal是目标。那么与环境进行互动的主体称为agent。

这个agent的中文翻译很成问题。有人翻译为“代理”，比较接近于它字面上的意思；有人翻译为“主体”，也就比较通俗；还有的翻译为“智能体”，就是比较贴切于人工智能了。那么我实际上不会去翻译这个词，很多时候就直接称为agent。不过从中文的理解上来说，我们知道agent这个词它有“特工”的含义，这样理解其实比较有意思。另外，我更倾向于将agent理解为“玩家”，强化学习的过程在很大程度上就像是一场游戏。强化学习现在最为领先的应用领域也就是棋牌游戏，包括最开始的Atari游戏，以及强化学习的封神之战——围棋，到最后的DOTA、星际争霸等等。

那么从学习的角度上来说呢，我觉得就把强化学习看成一场游戏更有意思一些，agent也就可以理解为玩家。当然这不是很重要了，重要的是理解强化学习的第一层结构，也就是基本元素：agent玩家、environment环境以及goal目标。强化学习是玩家在与环境的互动当中，为了达成一个目标而进行的学习过程。

有了玩家和环境，环境可以说就是这个游戏本身。那么这个游戏的玩法是什么？玩家的目标又是什么呢？这就是强化学习的第二层结构，我称之为主要元素，包括：第一，state状态；第二，action行动；第三，reward奖励。为什么称之为主要元素呢？因为整个强化学习的过程就是围绕着这三个元素展开的。

具体来说，首先玩家和环境会处于某种状态state。这个状态的含义很广泛，可以说包括了所有的相关信息。如果说这个游戏是《英雄联盟》，那么状态就应该包括敌方和队友的位置、等级、技能、双方的经济、野区的情况以及玩家自己的情况等等等等。如果说这个游戏是一场篮球比赛，那么状态就应该包括所有球员的位置、速度、球在谁手上、地板滑不滑等等等等。也就是说状态是可以很复杂的。那么最后一个简单的例子：围棋。围棋的状态非常简单，也就是棋盘上361个落子点的状态的整体。对于每一个落子点来说，可以有黑棋、白棋、空三种状态，那么整个围棋的状态在理论上则有3^361次方。虽然这是一个巨大的天文数字，但总的来说围棋的状态是很容易去分析的。

继续以围棋为例，在一个状态之下，玩家需要做出某种行动，也就是action。比如黑棋先手，当前的状态是棋盘上没有落子，黑棋则可以采取361种可能的行动，可以在任何一个位置落子。当黑棋采取了某一行动之后，比如黑棋走了星位，状态将会发生变化，比如白棋同样走星位，那么这就进入了下一个状态。黑棋再一次走出行动，state和action（状态和行动）的往复就构成了强化学习的主体部分。

那么什么是reward（奖励）呢？reward是指agent在一个状态之下采取了特定的行动之后所得到的及时的反馈。在强化学习中，reward通常是一个实数，并且可能是零。比如在围棋中，玩家的目标是赢得棋局，那么只有在达到赢棋的状态时才会有一个大于零的奖励。我们可以规定赢棋的奖励为1，输棋或者和棋的奖励为0，而在棋局结束之前，任何一次行动实际上得到的奖励都为0。

如果是在一场篮球比赛中，玩家的目标仍然是赢得比赛，不过获胜的条件变成了得分数大于对手。那么我们可以规定奖励为己方投篮得分数，以及对方投篮得分的相反数。总的来说，奖励应该是由最终的目标所决定的。如果在围棋中对吃掉对方的子进行奖励，那么强化学习的结果就会倾向于吃掉对方的子，而围棋获胜的条件是围地而不是吃子。一味地吃子可能适得其反。所以根据最终的目标合理地设置奖励，对于强化学习来说是很重要的。

反之，强化学习的目的则是最大化总的奖励，也就是整个游戏过程中所获得的奖励之和。奖励是一个及时的反馈，而目标是一个长远的结果，这两者之间的关系是需要理解清楚的。

那么以上就是强化学习的第二层结构，我称之为主要元素，包括：第一，state状态；第二，action行动；第三，reward奖励。

最后则是强化学习的第三层结构，我们称之为核心元素，一共有两个：policy（策略）以及value（价值）。策略很好理解，就是指在某一个状态下应该采取什么样的行动。那么简单地说，在数学上，策略其实就是一个函数，它的自变量或者说输入是一个状态，而因变量或者说输出则是一个行动。在围棋中，将当前棋盘的状态告诉这个策略函数，它总会告诉你下一步应该在哪里落子。很显然，强化学习想要达到的最终效果就是一个好的策略。如果你能拿到AlphaGo的策略函数，那么可以说你已经无敌了。

所以说，policy（策略）是强化学习的核心元素之一。那么什么是value（价值）呢？价值同样是一个函数，并且策略函数就取决于价值函数。所以毫无疑问，价值也是强化学习的核心元素。

价值函数通常有两种：第一种称为state value（状态价值函数）。顾名思义，它的输入是一个状态，而输出则是一个实数，这个实数就称为这个状态的价值。价值的含义很关键，它指的是预期将来会得到的所有奖励之和，也就是说，处于当前这一状态的情况下，玩家在将来能够得到的所有奖励的一个期望值。注意，玩家的目标就是得到的奖励之和尽可能大。因此，通过状态价值函数，玩家应该选择进入价值尽可能大的状态，而这是通过特定的行动来实现的。这就是状态价值函数决定了玩家的策略。

另一种价值函数称为state-action value（状态行动价值函数）。顾名思义，它指的不单单是一个状态所对应的价值，而是在特定状态下采取某种行动所具有的价值。同样，价值指的是在将来能够得到的所有奖励的一个期望值。那么显然，在一个特定的状态下，根据状态行动价值函数，玩家应该选择价值最大的那一个行动。这就是状态行动价值函数决定了玩家的策略。

综上大家应该理解了，为什么policy（策略）和value（价值）是强化学习的核心元素。强化学习所要学习的东西实际上就是一个好的价值函数，而一个好的价值函数决定一个好的策略。当然，有一部分算法可以不依赖于价值，直接学习策略，不过主流的或者说核心的强化学习算法通常都是基于价值的。这也是我们以后将要讲解的主要内容。

那么本期视频就到这里。我们主要分析了强化学习的结构。首先，什么是强化学习？强化学习是agent在与环境的互动当中，为了达成一个目标而进行的学习过程。我把这称之为强化学习的第一层结构，也就是基本元素，包括：第一，agent玩家；第二，environment环境；第三，goal目标。

强化学习的第二层结构我们称之为主要元素，包括：第一，state状态；第二，action行动；第三，reward奖励。

最后则是强化学习的第三层结构，我称之为核心元素：policy策略和value价值。这三层结构或者说这些组成元素之间的关系一定要理解清楚。